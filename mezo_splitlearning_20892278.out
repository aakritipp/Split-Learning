Starting Split Learning Job
================================
Job ID: 20892278
Node: skl-a-48
Time: Tue Sep 23 04:54:52 AM EDT 2025
Working Directory: /home/al1745/Split-Learning/large_models
================================
All required files present
Configuration:
   Model: facebook/opt-125m
   Epochs: 1
   Batch Size: 16
   Max Length: 512
   Learning Rate: 1e-4

Starting server...
Server PID: 2478522
Waiting 45 seconds for server initialization...
Server is running
Starting client...
Waiting for server to finish...
STARTING ENHANCED SPLIT LEARNING SERVER
============================================================
  Configuration:
   Model: facebook/opt-125m
   Batch size: 16
   Max length: 512
   ZOO server: False
   ZOO client: False
   F1 method: micro
  Using device: cuda
  Loading tokenizer: facebook/opt-125m
  Tokenizer loaded successfully
  Creating models...
Server owns layers [0, 5] with LoRA r=8, alpha=16
Full model ready; prefix-aware eval OFF by default (legacy behavior).
  Models created and synchronized
 Creating dataloaders...
 Creating dataloaders for task: sst2
Loading SST2 train dataset...
Processing 1000 sst2 examples...
Loaded 1000 sst2 examples
Loading SST2 validation dataset...
Processing 872 sst2 examples...
Loaded 872 sst2 examples
  Dataloaders created successfully
  Setting up optimizer...
✅ Optimizer ready
Setting up network...
  Debug - Server model info:
============================================================
SERVER READY - WAITING FOR CLIENT
============================================================
Server listening on localhost:12345
Start client with same parameters
============================================================
✅ Client connected from ('127.0.0.1', 57392)
Received client config: {'model_name': 'facebook/opt-125m', 'num_prefix': 5, 'lr': 0.0001, 'client_sgd': False, 'zoo_lr': 0.0001, 'mu': 0.0005, 'num_pert': 10, 'client_tuning': 'lora', 'lora': {'r': 8, 'alpha': 16, 'dropout': 0.0, 'targets': 'q_proj,v_proj'}}
Starting training...

Epoch 1/1
   Starting split learning training (SGD)...
   Training configuration:
   Max steps: 1000
   Batch size: 16
   Learning rate: 0.0001
   Eval every: 1000 steps
